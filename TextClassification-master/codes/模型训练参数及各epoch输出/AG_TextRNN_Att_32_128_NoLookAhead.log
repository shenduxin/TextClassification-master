Loading data...
Vocab size: 10002
Time usage: 0:00:02
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=10001)
  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (tanh1): Tanh()
  (tanh2): Tanh()
  (fc1): Linear(in_features=256, out_features=64, bias=True)
  (fc): Linear(in_features=64, out_features=4, bias=True)
)>
Iter:      0,  Train Loss:   1.4,  Train Acc: 32.03%,  Val Loss:   1.4,  Val Acc: 25.00%,  Time: 0:00:00 *
Iter:    100,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.57,  Val Acc: 78.08%,  Time: 0:00:03 *
Iter:    200,  Train Loss:  0.47,  Train Acc: 81.25%,  Val Loss:  0.45,  Val Acc: 83.45%,  Time: 0:00:05 *
Iter:    300,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:   0.4,  Val Acc: 85.18%,  Time: 0:00:08 *
Iter:    400,  Train Loss:  0.29,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 84.90%,  Time: 0:00:10 
Iter:    500,  Train Loss:  0.33,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 87.08%,  Time: 0:00:12 *
Iter:    600,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 87.13%,  Time: 0:00:14 
Iter:    700,  Train Loss:  0.25,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 88.23%,  Time: 0:00:16 *
Iter:    800,  Train Loss:  0.31,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 88.20%,  Time: 0:00:18 
Iter:    900,  Train Loss:  0.38,  Train Acc: 90.62%,  Val Loss:  0.32,  Val Acc: 88.33%,  Time: 0:00:20 *
Iter:   1000,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 88.22%,  Time: 0:00:22 
Iter:   1100,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.34,  Val Acc: 87.72%,  Time: 0:00:25 
Iter:   1200,  Train Loss:  0.25,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 88.62%,  Time: 0:00:27 
Iter:   1300,  Train Loss:  0.15,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 88.73%,  Time: 0:00:29 
Iter:   1400,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 88.85%,  Time: 0:00:31 *
Iter:   1500,  Train Loss:  0.12,  Train Acc: 96.09%,  Val Loss:  0.34,  Val Acc: 88.73%,  Time: 0:00:33 
Iter:   1600,  Train Loss:  0.26,  Train Acc: 87.50%,  Val Loss:  0.31,  Val Acc: 89.18%,  Time: 0:00:35 *
Iter:   1700,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.31,  Val Acc: 89.22%,  Time: 0:00:37 
Iter:   1800,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 88.32%,  Time: 0:00:39 
Iter:   1900,  Train Loss:  0.16,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 88.45%,  Time: 0:00:41 
Iter:   2000,  Train Loss:  0.16,  Train Acc: 94.53%,  Val Loss:  0.34,  Val Acc: 88.98%,  Time: 0:00:42 
Iter:   2100,  Train Loss: 0.091,  Train Acc: 96.88%,  Val Loss:  0.39,  Val Acc: 87.83%,  Time: 0:00:45 
Iter:   2200,  Train Loss:  0.17,  Train Acc: 92.97%,  Val Loss:   0.4,  Val Acc: 87.62%,  Time: 0:00:47 
Iter:   2300,  Train Loss:  0.15,  Train Acc: 93.75%,  Val Loss:  0.35,  Val Acc: 88.62%,  Time: 0:00:50 
Iter:   2400,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.36,  Val Acc: 88.72%,  Time: 0:00:52 
Iter:   2500,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 88.63%,  Time: 0:00:55 
Iter:   2600,  Train Loss:  0.14,  Train Acc: 96.88%,  Val Loss:  0.34,  Val Acc: 89.08%,  Time: 0:00:58 
No optimization for a long time, auto-stopping...
Test Loss:   0.3,  Test Acc: 89.63%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

       World     0.9101    0.9058    0.9079      1900
      Sports     0.9450    0.9584    0.9517      1900
    Business     0.8690    0.8516    0.8602      1900
    Sci/Tech     0.8604    0.8695    0.8649      1900

    accuracy                         0.8963      7600
   macro avg     0.8961    0.8963    0.8962      7600
weighted avg     0.8961    0.8963    0.8962      7600

Confusion Matrix...
[[1721   52   65   62]
 [  31 1821   26   22]
 [  71   27 1618  184]
 [  68   27  153 1652]]
Time usage: 0:00:00
