Loading data...
Time usage: 0:00:33
<bound method Module.parameters of Model(
  (xlnet): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (fc): Linear(in_features=768, out_features=4, bias=True)
)>
Iter:      0,  Train Loss:   1.9,  Train Acc: 25.00%,  Val Loss:   1.6,  Val Acc: 24.05%,  Time: 0:00:14 *
Iter:    100,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.27,  Val Acc: 90.67%,  Time: 0:01:05 *
Iter:    200,  Train Loss:  0.12,  Train Acc: 93.75%,  Val Loss:  0.28,  Val Acc: 89.98%,  Time: 0:01:54 
Iter:    300,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.25,  Val Acc: 91.32%,  Time: 0:02:46 *
Iter:    400,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.23,  Val Acc: 91.98%,  Time: 0:03:37 *
Iter:    500,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.22,  Val Acc: 92.10%,  Time: 0:04:28 *
Iter:    600,  Train Loss:  0.37,  Train Acc: 90.62%,  Val Loss:  0.23,  Val Acc: 92.18%,  Time: 0:05:18 
Iter:    700,  Train Loss:  0.29,  Train Acc: 87.50%,  Val Loss:  0.21,  Val Acc: 92.47%,  Time: 0:06:10 *
Iter:    800,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.21,  Val Acc: 92.50%,  Time: 0:07:02 *
Iter:    900,  Train Loss:  0.16,  Train Acc: 96.88%,  Val Loss:  0.22,  Val Acc: 92.02%,  Time: 0:07:52 
Iter:   1000,  Train Loss:  0.11,  Train Acc: 95.31%,  Val Loss:   0.2,  Val Acc: 92.75%,  Time: 0:08:44 *
Iter:   1100,  Train Loss:  0.16,  Train Acc: 92.19%,  Val Loss:  0.21,  Val Acc: 92.25%,  Time: 0:09:34 
Iter:   1200,  Train Loss:  0.15,  Train Acc: 95.31%,  Val Loss:   0.2,  Val Acc: 92.53%,  Time: 0:10:24 
Iter:   1300,  Train Loss: 0.068,  Train Acc: 98.44%,  Val Loss:   0.2,  Val Acc: 92.72%,  Time: 0:11:16 *
Iter:   1400,  Train Loss:  0.32,  Train Acc: 87.50%,  Val Loss:   0.2,  Val Acc: 92.45%,  Time: 0:12:06 
Iter:   1500,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.19,  Val Acc: 93.25%,  Time: 0:12:57 *
Iter:   1600,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.19,  Val Acc: 93.23%,  Time: 0:13:47 
Iter:   1700,  Train Loss:  0.14,  Train Acc: 92.19%,  Val Loss:   0.2,  Val Acc: 92.27%,  Time: 0:14:37 
Iter:   1800,  Train Loss:  0.18,  Train Acc: 95.31%,  Val Loss:   0.2,  Val Acc: 92.93%,  Time: 0:15:27 
Iter:   1900,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.21,  Val Acc: 92.60%,  Time: 0:16:17 
Iter:   2000,  Train Loss:  0.18,  Train Acc: 92.19%,  Val Loss:  0.19,  Val Acc: 93.52%,  Time: 0:17:09 *
Iter:   2100,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.18,  Val Acc: 93.40%,  Time: 0:18:00 *
Iter:   2200,  Train Loss:  0.11,  Train Acc: 93.75%,  Val Loss:  0.18,  Val Acc: 93.27%,  Time: 0:18:50 
Iter:   2300,  Train Loss: 0.098,  Train Acc: 95.31%,  Val Loss:  0.19,  Val Acc: 93.47%,  Time: 0:19:40 
Iter:   2400,  Train Loss:  0.11,  Train Acc: 93.75%,  Val Loss:  0.18,  Val Acc: 93.43%,  Time: 0:20:30 
Iter:   2500,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.18,  Val Acc: 93.67%,  Time: 0:21:20 
Iter:   2600,  Train Loss:  0.15,  Train Acc: 95.31%,  Val Loss:  0.18,  Val Acc: 93.42%,  Time: 0:22:10 
Iter:   2700,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.21,  Val Acc: 92.25%,  Time: 0:23:00 
Iter:   2800,  Train Loss:  0.16,  Train Acc: 95.31%,  Val Loss:   0.2,  Val Acc: 93.00%,  Time: 0:23:50 
Iter:   2900,  Train Loss:  0.15,  Train Acc: 93.75%,  Val Loss:  0.18,  Val Acc: 93.60%,  Time: 0:24:41 *
Iter:   3000,  Train Loss:  0.12,  Train Acc: 95.31%,  Val Loss:  0.19,  Val Acc: 92.97%,  Time: 0:25:31 
Iter:   3100,  Train Loss:  0.25,  Train Acc: 89.06%,  Val Loss:  0.18,  Val Acc: 93.47%,  Time: 0:26:23 *
Iter:   3200,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:  0.17,  Val Acc: 93.92%,  Time: 0:27:15 *
Iter:   3300,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.18,  Val Acc: 93.47%,  Time: 0:28:05 
Iter:   3400,  Train Loss:  0.21,  Train Acc: 92.19%,  Val Loss:  0.18,  Val Acc: 93.45%,  Time: 0:28:55 
Iter:   3500,  Train Loss:   0.3,  Train Acc: 87.50%,  Val Loss:  0.18,  Val Acc: 93.77%,  Time: 0:29:45 
Iter:   3600,  Train Loss:  0.19,  Train Acc: 92.19%,  Val Loss:  0.19,  Val Acc: 92.53%,  Time: 0:30:35 
Iter:   3700,  Train Loss:  0.15,  Train Acc: 95.31%,  Val Loss:  0.19,  Val Acc: 93.60%,  Time: 0:31:25 
Iter:   3800,  Train Loss: 0.049,  Train Acc: 98.44%,  Val Loss:  0.18,  Val Acc: 93.48%,  Time: 0:32:15 
Iter:   3900,  Train Loss:  0.19,  Train Acc: 92.19%,  Val Loss:  0.18,  Val Acc: 93.80%,  Time: 0:33:05 
Iter:   4000,  Train Loss: 0.075,  Train Acc: 96.88%,  Val Loss:  0.18,  Val Acc: 93.32%,  Time: 0:33:55 
Iter:   4100,  Train Loss:  0.24,  Train Acc: 90.62%,  Val Loss:  0.19,  Val Acc: 93.40%,  Time: 0:34:45 
Iter:   4200,  Train Loss: 0.089,  Train Acc: 96.88%,  Val Loss:  0.18,  Val Acc: 93.47%,  Time: 0:35:35 
No optimization for a long time, auto-stopping...
Test Loss:  0.18,  Test Acc: 94.13%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

       World     0.9340    0.9616    0.9476      1900
      Sports     0.9899    0.9795    0.9847      1900
    Business     0.9247    0.9042    0.9143      1900
    Sci/Tech     0.9171    0.9200    0.9185      1900

    accuracy                         0.9413      7600
   macro avg     0.9414    0.9413    0.9413      7600
weighted avg     0.9414    0.9413    0.9413      7600

Confusion Matrix...
[[1827    9   34   30]
 [  23 1861   11    5]
 [  56    3 1718  123]
 [  50    7   95 1748]]
Time usage: 0:00:15
