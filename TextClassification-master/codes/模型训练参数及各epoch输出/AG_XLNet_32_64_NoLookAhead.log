Loading data...
Time usage: 0:00:34
<bound method Module.parameters of Model(
  (xlnet): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (fc): Linear(in_features=768, out_features=4, bias=True)
)>
Iter:      0,  Train Loss:   2.3,  Train Acc: 23.44%,  Val Loss:   1.7,  Val Acc: 23.05%,  Time: 0:00:08 *
Iter:    100,  Train Loss:  0.49,  Train Acc: 85.94%,  Val Loss:  0.34,  Val Acc: 88.28%,  Time: 0:00:36 *
Iter:    200,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 89.18%,  Time: 0:01:05 *
Iter:    300,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:   0.3,  Val Acc: 89.43%,  Time: 0:01:32 *
Iter:    400,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.29,  Val Acc: 89.85%,  Time: 0:02:00 *
Iter:    500,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.27,  Val Acc: 90.38%,  Time: 0:02:29 *
Iter:    600,  Train Loss:  0.35,  Train Acc: 92.19%,  Val Loss:  0.27,  Val Acc: 90.60%,  Time: 0:02:55 
Iter:    700,  Train Loss:  0.36,  Train Acc: 84.38%,  Val Loss:  0.26,  Val Acc: 90.73%,  Time: 0:03:23 *
Iter:    800,  Train Loss:  0.28,  Train Acc: 92.19%,  Val Loss:  0.26,  Val Acc: 90.88%,  Time: 0:03:51 *
Iter:    900,  Train Loss:  0.37,  Train Acc: 85.94%,  Val Loss:  0.28,  Val Acc: 90.30%,  Time: 0:04:18 
Iter:   1000,  Train Loss:  0.13,  Train Acc: 98.44%,  Val Loss:  0.24,  Val Acc: 91.28%,  Time: 0:04:46 *
Iter:   1100,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.25,  Val Acc: 90.77%,  Time: 0:05:13 
Iter:   1200,  Train Loss:  0.16,  Train Acc: 95.31%,  Val Loss:  0.24,  Val Acc: 91.22%,  Time: 0:05:41 *
Iter:   1300,  Train Loss:  0.11,  Train Acc: 96.88%,  Val Loss:  0.24,  Val Acc: 91.37%,  Time: 0:06:10 *
Iter:   1400,  Train Loss:  0.32,  Train Acc: 87.50%,  Val Loss:  0.25,  Val Acc: 90.72%,  Time: 0:06:37 
Iter:   1500,  Train Loss:  0.27,  Train Acc: 87.50%,  Val Loss:  0.23,  Val Acc: 91.75%,  Time: 0:07:05 *
Iter:   1600,  Train Loss:  0.14,  Train Acc: 96.88%,  Val Loss:  0.24,  Val Acc: 91.27%,  Time: 0:07:32 
Iter:   1700,  Train Loss:  0.15,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 91.70%,  Time: 0:07:58 
Iter:   1800,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 91.62%,  Time: 0:08:25 
Iter:   1900,  Train Loss:  0.29,  Train Acc: 85.94%,  Val Loss:  0.24,  Val Acc: 90.75%,  Time: 0:08:51 
Iter:   2000,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 91.88%,  Time: 0:09:18 
Iter:   2100,  Train Loss:  0.32,  Train Acc: 85.94%,  Val Loss:  0.22,  Val Acc: 91.72%,  Time: 0:09:46 *
Iter:   2200,  Train Loss:  0.17,  Train Acc: 96.88%,  Val Loss:  0.22,  Val Acc: 91.93%,  Time: 0:10:12 
Iter:   2300,  Train Loss:  0.17,  Train Acc: 90.62%,  Val Loss:  0.22,  Val Acc: 92.13%,  Time: 0:10:39 
Iter:   2400,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.22,  Val Acc: 92.30%,  Time: 0:11:07 *
Iter:   2500,  Train Loss:  0.16,  Train Acc: 90.62%,  Val Loss:  0.23,  Val Acc: 91.93%,  Time: 0:11:33 
Iter:   2600,  Train Loss:  0.15,  Train Acc: 95.31%,  Val Loss:  0.24,  Val Acc: 91.20%,  Time: 0:11:59 
Iter:   2700,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.25,  Val Acc: 90.73%,  Time: 0:12:25 
Iter:   2800,  Train Loss:  0.25,  Train Acc: 93.75%,  Val Loss:  0.25,  Val Acc: 90.80%,  Time: 0:12:52 
Iter:   2900,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.23,  Val Acc: 91.42%,  Time: 0:13:18 
Iter:   3000,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 91.47%,  Time: 0:13:44 
Iter:   3100,  Train Loss:  0.39,  Train Acc: 84.38%,  Val Loss:  0.22,  Val Acc: 91.87%,  Time: 0:14:10 
Iter:   3200,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.22,  Val Acc: 91.82%,  Time: 0:14:36 
Iter:   3300,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:  0.21,  Val Acc: 92.28%,  Time: 0:15:04 *
Iter:   3400,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 91.73%,  Time: 0:15:30 
Iter:   3500,  Train Loss:   0.3,  Train Acc: 92.19%,  Val Loss:  0.22,  Val Acc: 92.38%,  Time: 0:15:56 
Iter:   3600,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 91.32%,  Time: 0:16:23 
Iter:   3700,  Train Loss:  0.17,  Train Acc: 96.88%,  Val Loss:  0.23,  Val Acc: 91.50%,  Time: 0:16:49 
Iter:   3800,  Train Loss: 0.082,  Train Acc: 98.44%,  Val Loss:  0.23,  Val Acc: 91.60%,  Time: 0:17:15 
Iter:   3900,  Train Loss:  0.19,  Train Acc: 92.19%,  Val Loss:  0.22,  Val Acc: 91.97%,  Time: 0:17:41 
Iter:   4000,  Train Loss: 0.098,  Train Acc: 96.88%,  Val Loss:  0.23,  Val Acc: 92.20%,  Time: 0:18:08 
Iter:   4100,  Train Loss:   0.3,  Train Acc: 89.06%,  Val Loss:  0.24,  Val Acc: 91.75%,  Time: 0:18:34 
Iter:   4200,  Train Loss: 0.069,  Train Acc: 96.88%,  Val Loss:  0.22,  Val Acc: 92.25%,  Time: 0:19:00 
Iter:   4300,  Train Loss:  0.17,  Train Acc: 92.19%,  Val Loss:  0.23,  Val Acc: 91.68%,  Time: 0:19:27 
No optimization for a long time, auto-stopping...
Test Loss:  0.21,  Test Acc: 93.03%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

       World     0.9443    0.9374    0.9408      1900
      Sports     0.9723    0.9800    0.9761      1900
    Business     0.9081    0.8942    0.9011      1900
    Sci/Tech     0.8963    0.9095    0.9028      1900

    accuracy                         0.9303      7600
   macro avg     0.9302    0.9303    0.9302      7600
weighted avg     0.9302    0.9303    0.9302      7600

Confusion Matrix...
[[1781   28   41   50]
 [  14 1862   11   13]
 [  51   13 1699  137]
 [  40   12  120 1728]]
Time usage: 0:00:07
